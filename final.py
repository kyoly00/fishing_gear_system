import sys
sys.path.append(r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\ì¡°ì—…í–‰íƒœëª¨ë¸')

from cal_ais_values import cal_val, cal_statics
# from load_test_model_1dcnn import CustomModel, CustomDataset  # TCN ê¸°ë°˜ ëª¨ë¸/ë°ì´í„°ì…‹
from model_def import CustomModel, CustomDataset  # TCN ê¸°ë°˜ ëª¨ë¸/ë°ì´í„°ì…‹




import json
import pandas as pd
import copy
import pickle
import os
from os.path import join
from os import listdir
import numpy as np
from datetime import datetime, timedelta
from os.path import split
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')



#-------------------------------
# ë°ì´í„°ì…‹ ì¤€ë¹„
#-------------------------------

# 1. íŒŒì¼ëª… ì¼ë¶€ë¡œ ìë™ ë¡œë“œ í•¨ìˆ˜
def load_csv_from_partial_name(directory, partial_filename):
    """
    ì§€ì •í•œ í´ë” ì•ˆì—ì„œ partial_filenameì„ í¬í•¨í•˜ëŠ” ì²« ë²ˆì§¸ CSV íŒŒì¼ì„ ì°¾ì•„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    """
    for file in os.listdir(directory):
        if partial_filename in file and file.endswith('.csv'):
            full_path = os.path.join(directory, file)
            print(f"âœ… íŒŒì¼ ë¡œë“œ: {full_path}")
            df = pd.read_csv(full_path, encoding="UTF-8")
            return full_path, df
    raise FileNotFoundError(f"âŒ '{partial_filename}'ë¥¼ í¬í•¨í•œ .csv íŒŒì¼ì„ '{directory}'ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì‚¬ìš©ì ì„¤ì •
jsondict_pkl = r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\ì¡°ì—…í–‰íƒœëª¨ë¸\jsondict_v0.2.pkl'
gps_folder = r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\GPS_DATA'





#======================================================
filename_hint = '33_5__128_5__2022-03-10'  # íŒŒì¼ëª… ì¼ë¶€
#======================================================



cols2 = ['sea_surface_temperature','sea_surface_salinity','current_speed','wind','tide','bottom_depth','chlorophyll','DIN','DIP','dissolved_oxygen','fishery_density','fishery_type','fishery_behavior']
static_cols = ['mean_ship_course_change','standard_deviation_of_ship_course_change','histogram_of_ship_course_change','mean_ship_course_change_per_velocity_stage',
               'mean_velocity_change','standard_deviation_of_velocity_change','mean_velocity','histogram_of_velocity','histogram_of_velocity_change',
               'velocity_change_per_velocity_stage']


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. JSON dict ë¡œë“œ
with open(jsondict_pkl, 'rb') as f:
    features_value_dict = pickle.load(f)
    geojson_dict = pickle.load(f)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. CSV ìë™ ë¡œë“œ ë° ì²˜ë¦¬
csv_path, csv_data = load_csv_from_partial_name(gps_folder, filename_hint)
csv_time = pd.to_datetime(csv_data['datetime'])



# 5. í†µê³„ ê³„ì‚° ë° geojson feature ì±„ìš°ê¸°
total_statics = cal_statics(csv_data)  # â†’ static_cols ê¸°ì¤€ 10ê°œ ë¦¬ìŠ¤íŠ¸


features_value_list = []

for i in range(len(csv_data)) :
    for ais_i in range(len(static_cols)):
        features_value_dict['properties'][static_cols[ais_i]]=total_statics[i][ais_i]
    temp_dict = copy.deepcopy(features_value_dict)
    features_value_list.append(temp_dict)
geojson_dict['features'] = features_value_list



# ì›ë˜ AIS ë°ì´í„°í”„ë ˆì„ì— í†µê³„ ì—´ ì¶”ê°€
for i, col in enumerate(static_cols):
    csv_data[col] = [row[i] for row in total_statics]




#----------------------------------------------

import ast

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì»¬ëŸ¼ëª… ë³€ê²½
df2 = csv_data.rename(columns={
    'datetime': 'time_stamp',
    'lat': 'latitude',
    'lon': 'longitude'
})

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ë¦¬ìŠ¤íŠ¸í˜• ì»¬ëŸ¼ ë¶„í•´ í•¨ìˆ˜
def expand_list_column(df, column_name, new_column_prefix, target_length):
    def parse_list(val):
        if isinstance(val, list):
            return val
        if isinstance(val, str):
            try:
                return ast.literal_eval(val)
            except:
                return [0] * target_length
        return [0] * target_length

    parsed_lists = df[column_name].apply(parse_list)
    for i in range(target_length):
        df[f'{new_column_prefix}{i+1}'] = parsed_lists.apply(lambda x: x[i] if i < len(x) else 0)

    df = df.drop(columns=[column_name])
    return df

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ë¦¬ìŠ¤íŠ¸í˜• ì»¬ëŸ¼ë“¤ í™•ì¥
df2 = expand_list_column(df2, 'histogram_of_ship_course_change', 'histogram_of_ship_course_change', 12)
df2 = expand_list_column(df2, 'mean_ship_course_change_per_velocity_stage', 'mean_ship_course_change_per_velocity_stage', 3)
df2 = expand_list_column(df2, 'histogram_of_velocity', 'histogram_of_velocity', 7)
df2 = expand_list_column(df2, 'histogram_of_velocity_change', 'histogram_of_velocity_change', 8)
df2 = expand_list_column(df2, 'velocity_change_per_velocity_stage', 'velocity_change_per_velocity_stage', 3)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. x_cols ì •ì˜
x_cols = [
    'time_stamp', 'latitude', 'longitude', 'sea_surface_temperature', 'sea_surface_salinity',
    'current_speed1', 'current_speed2', 'wind1', 'wind2', 'tide1', 'tide2', 'bottom_depth',
    'chlorophyll', 'DIN', 'DIP', 'dissolved_oxygen',
    'fishery_density1', 'fishery_density2', 'fishery_density3', 'fishery_density4',
    'fishery_density5', 'fishery_density6', 'fishery_density7',
    'fishery_type', 'fishery_behavior', 'month', 'hour',
    'mean_ship_course_change', 'standard_deviation_of_ship_course_change',
    'histogram_of_ship_course_change1', 'histogram_of_ship_course_change2',
    'histogram_of_ship_course_change3', 'histogram_of_ship_course_change4',
    'histogram_of_ship_course_change5', 'histogram_of_ship_course_change6',
    'histogram_of_ship_course_change7', 'histogram_of_ship_course_change8',
    'histogram_of_ship_course_change9', 'histogram_of_ship_course_change10',
    'histogram_of_ship_course_change11', 'histogram_of_ship_course_change12',
    'mean_ship_course_change_per_velocity_stage1', 'mean_ship_course_change_per_velocity_stage2',
    'mean_ship_course_change_per_velocity_stage3',
    'mean_velocity_change', 'standard_deviation_of_velocity_change', 'mean_velocity',
    'histogram_of_velocity1', 'histogram_of_velocity2', 'histogram_of_velocity3',
    'histogram_of_velocity4', 'histogram_of_velocity5', 'histogram_of_velocity6',
    'histogram_of_velocity7',
    'histogram_of_velocity_change1', 'histogram_of_velocity_change2',
    'histogram_of_velocity_change3', 'histogram_of_velocity_change4',
    'histogram_of_velocity_change5', 'histogram_of_velocity_change6',
    'histogram_of_velocity_change7', 'histogram_of_velocity_change8',
    'velocity_change_per_velocity_stage1', 'velocity_change_per_velocity_stage2',
    'velocity_change_per_velocity_stage3',
    'observed_fishing_type', 'observed_fishing_info', 'px', 'py', 'filename'
]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ëˆ„ë½ëœ ì»¬ëŸ¼ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°
for col in x_cols:
    if col not in df2.columns:
        df2[col] = 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. ìµœì¢… ì»¬ëŸ¼ ìˆœì„œ ì •ë ¬
df2 = df2[x_cols]

# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # 8. ê²°ê³¼ ì €ì¥ (ì„ íƒì‚¬í•­)
# df2.to_csv('ì›ë³¸ë°ì´í„°_êµ¬ì¡°ë§ì¶”ê¸°.csv', index=False)



#---------------------------------------------------------------------
# í•™ìŠµ ëª¨ë¸

import torch
from torch.utils.data import DataLoader
import datetime as dt

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ëª¨ë¸ ë¡œë“œ
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = CustomModel().to(device)
ckpt = torch.load(r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\ì¡°ì—…í–‰íƒœëª¨ë¸\new_29_acuur_0_8100', map_location=device)
sd   = ckpt.get('model_state_dict', ckpt)
model.load_state_dict(sd)
model.eval()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ì „ì²˜ë¦¬ + ì˜ˆì¸¡
dataset = CustomDataset(df2)
dataloader = DataLoader(dataset, batch_size=32, shuffle=False)

all_preds = []
for xb, _, _, _ in dataloader:  # 4ê°œ ë°›ì•„ì„œ ì²« ë²ˆì§¸ë§Œ ì‚¬ìš©
    xb = xb.to(device)
    xb = xb.permute(0, 2, 1)  # (B, 68, 1440)
    with torch.no_grad():
        logits = model(xb)        # (B, 1440, 4)
        preds = logits.argmax(dim=2).cpu().numpy()  # (B, 1440)
        all_preds.extend(preds)

# for xb, _ in dataloader:
#     xb = xb.to(device)
#     xb = xb.permute(0, 2, 1)  # (B, 68, 1440)
#     with torch.no_grad():
#         logits = model(xb)        # (B, 1440, 4)
#         preds = logits.argmax(dim=2).cpu().numpy()  # (B, 1440)
#         all_preds.extend(preds)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë³¸ CSVì— ë³‘í•©
flat_preds = np.array(all_preds).flatten()


df2['fishery_behavior'] = flat_preds[:len(df2)]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. ê²°ê³¼ ì €ì¥ (íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ê²½ë¡œ ì§€ì •)
output_dir = r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\label_dataset_csv'
os.makedirs(output_dir, exist_ok=True)

output_filename = f'{filename_hint}_label_data.csv'
output_path = os.path.join(output_dir, output_filename)

df2.to_csv(output_path, index=False, encoding='utf-8-sig')
print(f"âœ… ì˜ˆì¸¡ ë° ì €ì¥ ì™„ë£Œ: {output_path}")




#========================================================================
#========================================================================

import json

# CSV íŒŒì¼ ê²½ë¡œ
csv_path = os.path.join(output_dir, output_filename)
df = pd.read_csv(csv_path)

# ë¦¬ìŠ¤íŠ¸ë¡œ ë³‘í•©í•  í•„ë“œ ì •ì˜
list_fields = {
    "current_speed": [f"current_speed{i}" for i in range(1, 3)],
    "wind": [f"wind{i}" for i in range(1, 3)],
    "tide": [f"tide{i}" for i in range(1, 3)],
    "fishery_density": [f"fishery_density{i}" for i in range(1, 8)],
    "histogram_of_ship_course_change": [f"histogram_of_ship_course_change{i}" for i in range(1, 13)],
    "mean_ship_course_change_per_velocity_stage": [f"mean_ship_course_change_per_velocity_stage{i}" for i in range(1, 4)],
    "histogram_of_velocity": [f"histogram_of_velocity{i}" for i in range(1, 8)],
    "histogram_of_velocity_change": [f"histogram_of_velocity_change{i}" for i in range(1, 9)],
    "velocity_change_per_velocity_stage": [f"velocity_change_per_velocity_stage{i}" for i in range(1, 4)]
}

# features ìƒì„±
features = []
for _, row in df.iterrows():
    properties = {}

    # ì¼ë°˜ í•„ë“œ ë³µì‚¬
    for col in df.columns:
        if any(col in subcols for subcols in list_fields.values()):
            continue
        properties[col] = row[col] if not pd.isna(row[col]) else None

    # ë¦¬ìŠ¤íŠ¸ í•„ë“œ ë³‘í•©
    for field_name, subcols in list_fields.items():
        values = [row[c] if not pd.isna(row[c]) else 0 for c in subcols]
        properties[field_name] = values

    # geometry ìƒì„±
    feature = {
        "type": "Feature",
        "properties": properties,
        "geometry": {
            "type": "Point",
            "coordinates": [round(row["longitude"], 4), round(row["latitude"], 4)]
        },
    }
    features.append(feature)

# ì „ì²´ GeoJSON êµ¬ì¡°
geojson_output = {
    "type": "FeatureCollection",
    "name": f'{filename_hint}_label_data.geojson',
    "crs": {
        "filename": f'{filename_hint}_label_data.geojson',
        "start_time": str(df["time_stamp"].min()),
        "end_time": str(df["time_stamp"].max()),
        "weather": None,
        "AIS_porcessing_method": "Statistics",
        "phyiscial_information_processing_method": "Original",
        "biological_information_processing_method": "Original",
        "multiple_fishery_type": None,
        "nationality": None
    },
    "features": features
}


# 4. ê²°ê³¼ ì €ì¥ (íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ê²½ë¡œ ì§€ì •)
output_dir = r'C:\Users\ime\Desktop\ìœ ì‹¤_ìµœì¢…\label_dataset_geojson'
os.makedirs(output_dir, exist_ok=True)

output_filename = f'{filename_hint}_label_data.geojson'
output_path = os.path.join(output_dir, output_filename)

with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(geojson_output, f, ensure_ascii=False, indent=2)

print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")



#=================================================================================
#=================================================================================
#=================================================================================
#=================================================================================



import os
#â€“â€“ PROJ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (pyproj â‰¥3ìš© PROJ_DATA í¬í•¨)
os.environ['PROJ_LIB']  = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'
os.environ['PROJ_DATA'] = r'C:\Users\HUFS\anaconda3\envs\opendrift_env\Library\share\proj'

import json
import glob
import time
from datetime import datetime, timedelta


import csv
from dateutil import parser
import numpy as np
import pandas as pd
import xarray as xr
import cdsapi
import requests
import geopandas as gpd
from shapely.geometry import Point, LineString

import matplotlib.pyplot as plt
from scipy.interpolate import griddata

from opendrift.models.oceandrift import OceanDrift
from opendrift.readers import reader_netCDF_CF_generic
from collections import OrderedDict

import matplotlib.pyplot as plt
import matplotlib as mpl

plt.rcParams['font.family'] = 'Malgun Gothic'
mpl.rcParams['axes.unicode_minus'] = False



# í´ë” ê²½ë¡œ ë° íŒŒì¼ëª… ì™„ì„±
folder_path = r'.\label_dataset_geojson'
geojson_filename = f'{filename_hint}_label_data.geojson'
geojson_file = os.path.join(folder_path, geojson_filename)

# ì—ëŸ¬ ë¡œê·¸ ê²½ë¡œ
error_log_path = os.path.join(r'.\error_log', "error_log.csv")

# ì—ëŸ¬ ë¡œê·¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±
if not os.path.exists(error_log_path):
    with open(error_log_path, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["íŒŒì¼ëª…", "ì˜¤ë¥˜ì¢…ë¥˜", "ì˜¤ë¥˜ë©”ì‹œì§€"])

# ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬
try:
    geojson_filename = os.path.basename(geojson_file)
    visibility = None            # ê°€ì‹œê±°ë¦¬(m)
    distance_km = None           # ì¤‘ê°„ íˆ¬ë§ â†” ì–‘ë§ ê±°ë¦¬(km)
    prediction_result = "íŒë‹¨ ë¶ˆê°€"  # ì˜ˆì¸¡ ì„±ê³µ ì—¬ë¶€

    with open(geojson_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    # ì—¬ê¸°ì„œ ë°ì´í„° ì²˜ë¦¬ ë¡œì§ ì¶”ê°€
    print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ: {geojson_file}")



    ###############################################################################
    # PART 1: GeoJSON â†’ DataFrame (íˆ¬ë§ ê¶¤ì  ì¶”ì¶œ)
    ###############################################################################
    rows = []
    for feat in data.get("features", []):
        p = feat["properties"]
        beh = p.get("fishery_behavior")
        rows.append({
            "time_stamp": p["time_stamp"],
            "lon":         p["longitude"],
            "lat":         p["latitude"],
            "fishery_behavior": beh
        })

    df = pd.DataFrame(rows)
    df['time_stamp'] = pd.to_datetime(df['time_stamp'])
    df = df.sort_values('time_stamp').reset_index(drop=True)

    # íˆ¬ë§ ì‹œì‘ ì§€ì ë§Œ í•„í„°ë§ (1->3 ë˜ëŠ” 0->3 ë³€í™” ì‹œì )
    df['prev_behavior'] = df['fishery_behavior'].shift(1)
    drop_points = df[
        (df['fishery_behavior'] == 3) &
        (df['prev_behavior'] != 3)
    ].copy()

    df3 = df[df['fishery_behavior'] == 3].copy()
    if df3.empty or drop_points.empty:
        raise RuntimeError("íˆ¬ë§ êµ¬ê°„ ë˜ëŠ” ì‹œì‘ ì‹œì  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    lat_min = df3['lat'].min()
    lat_max = df3['lat'].max()
    lon_min = df3['lon'].min()
    lon_max = df3['lon'].max()
    start_time = df3['time_stamp'].min()
    end_limit  = df3['time_stamp'].max()

    first_time = pd.to_datetime(df3['time_stamp'].min())
    last_time  = pd.to_datetime(df3['time_stamp'].max())

    simulation_duration = last_time - start_time

    # ì—°, ì›”, ì¼ ë¬¸ìì—´ë¡œ ì¶”ì¶œ
    year  = f"{first_time.strftime('%Y')}"
    month = f"{first_time.strftime('%m')}"
    day   = f"{first_time.strftime('%d')}"

    lat_min = df3['lat'].min() - 0.3
    lat_max = df3['lat'].max() + 0.3
    lon_min = df3['lon'].min() - 0.3
    lon_max = df3['lon'].max() + 0.3





    #=======================================================
    # ğŸ“Œ 1. GeoJSON â†’ DataFrame
    #======================================================
    geojson_file
    with open(geojson_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    records = []
    for feature in data['features']:
        p = feature['properties']
        records.append({
            'time_stamp': p['time_stamp'],
            'lat': p['latitude'],
            'lon': p['longitude'],
            'fishery_behavior': p['fishery_behavior']
        })

    df = pd.DataFrame(records)
    df['time_stamp'] = pd.to_datetime(df['time_stamp'])

    # ğŸ“Œ 2. ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (1ì‹œê°„ ê°„ê²©)
    start_time = df['time_stamp'].min().replace(minute=0, second=0)
    end_time = df['time_stamp'].max()
    time_list = []
    current_time = start_time
    while current_time <= end_time:
        time_list.append(current_time)
        current_time += timedelta(hours=1)


    lat_grid = np.arange(round(lat_min, 2), round(lat_max, 2) + 0.01, 0.01)
    lon_grid = np.arange(round(lon_min, 2), round(lon_max, 2) + 0.01, 0.01)



    # ====== NetCDF íŒŒì¼ ê²½ë¡œ ë¯¸ë¦¬ ì„¤ì • ======
    input_basename = os.path.splitext(os.path.basename(geojson_file))[0]
    nc_folder = r".\KHOA_nc_data"
    os.makedirs(nc_folder, exist_ok=True)


        # ====== API í˜¸ì¶œ ë° ë³´ê°„ ìˆ˜í–‰ ======
    service_key = 'ANM8LV6zTsRNiGg6FCUMpw=='
    base_url = "http://www.khoa.go.kr/api/oceangrid/tidalCurrentAreaGeoJson/search.do"
    all_data = []


    output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")

    # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ìƒëµ ======
    if os.path.exists(output_path):
        print(f"ğŸ”„ ì´ë¯¸ NetCDF ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {output_path}")
    else:

    # ğŸ“Œ 5. API í˜¸ì¶œ ë° ë°ì´í„° ì €ì¥
        for t in time_list:
            params = {
                "DataType": "tidalCurrentAreaGeoJson",
                "ServiceKey": service_key,
                "Date": t.strftime("%Y%m%d"),
                "Hour": t.strftime("%H"),
                "Minute": "00",
                "MinX": lon_min,
                "MaxX": lon_max,
                "MinY": lat_min,
                "MaxY": lat_max,
                "Scale": 2000000
            }

            print(f"[ìš”ì²­] {params['Date']} {params['Hour']}ì‹œ")
            try:
                response = requests.get(base_url, params=params)
                if response.status_code == 200 and response.text.startswith('{'):
                    geojson_data = response.json()
                    for feature in geojson_data.get("features", []):
                        p = feature["properties"]
                        lat = p.get("lat")
                        lon = p.get("lon")
                        spd = p.get("current_speed")
                        direction = p.get("current_direct")
                        if None in (lat, lon, spd, direction):
                            continue
                        spd_m = spd / 100
                        rad = np.radians(direction)
                        u = spd_m * np.sin(rad)
                        v = spd_m * np.cos(rad)
                        all_data.append({
                            "time": t,
                            "lat": lat,
                            "lon": lon,
                            "u": u,
                            "v": v
                        })
                else:
                    print(f"âŒ API ì‹¤íŒ¨: status={response.status_code}")
            except Exception as e:
                print(f"[ì˜ˆì™¸] {e}")

        # ğŸ“Œ 6. ì •ë°©ê²©ì ë³´ê°„ ë° NetCDF ìƒì„±
        df_all = pd.DataFrame(all_data)
        times = sorted(df_all["time"].unique())
        u_interp = []
        v_interp = []

        lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)

        for t in times:
            sub = df_all[df_all["time"] == t]
            points = np.array(sub[["lon", "lat"]])
            u_vals = sub["u"].values
            v_vals = sub["v"].values

            u_grid = griddata(points, u_vals, (lon_mesh, lat_mesh), method='linear')
            v_grid = griddata(points, v_vals, (lon_mesh, lat_mesh), method='linear')

            u_interp.append(u_grid)
            v_interp.append(v_grid)

        # 7. OpenDrift ì¸ì‹ ê°€ëŠ¥í•˜ë„ë¡ ë³€ìˆ˜ëª… + ë©”íƒ€ë°ì´í„° ì„¤ì •
        ds = xr.Dataset(
            {
                "eastward_sea_water_velocity": (["time", "lat", "lon"], np.array(u_interp)),
                "northward_sea_water_velocity": (["time", "lat", "lon"], np.array(v_interp)),
            },
            coords={
                "time": times,
                "lat": lat_grid,
                "lon": lon_grid,
            },
            attrs={
                "title": "ì •ë°©ê²©ì ë³´ê°„ëœ KHOA í•´ë¥˜ ì˜ˆì¸¡ ë°ì´í„°",
                "source": "tidalCurrentAreaGeoJson API"
            }
        )

        # ë³€ìˆ˜ì— CF-convention ë©”íƒ€ë°ì´í„° ì¶”ê°€
        ds["eastward_sea_water_velocity"].attrs["standard_name"] = "eastward_sea_water_velocity"
        ds["eastward_sea_water_velocity"].attrs["units"] = "m s-1"
        ds["northward_sea_water_velocity"].attrs["standard_name"] = "northward_sea_water_velocity"
        ds["northward_sea_water_velocity"].attrs["units"] = "m s-1"

        os.makedirs(nc_folder, exist_ok=True)

        output_path = os.path.join(nc_folder, f"{input_basename}_uv.nc")
        ds.to_netcdf(output_path)
        print(f"âœ… NetCDF ì €ì¥ ì™„ë£Œ: {output_path}")





    #=======================================================
    # PART 3: ERA5 CDS API ë°ì´í„° ë‹¤ìš´ë¡œë“œã…¡
    #=======================================================

    # ë‚ ì§œ ë²”ìœ„ ìë™ ìƒì„± (ì˜ˆ: ['09','10','11'] ë“±)
    num_days = (last_time.date() - first_time.date()).days + 1
    days = [(first_time + timedelta(days=i)).strftime("%d") for i in range(num_days)]

    # ì‹œê°„ ë¦¬ìŠ¤íŠ¸ (00:00 ~ 23:00)
    times = [f"{h:02d}:00" for h in range(24)]


    area   = [lat_max, lon_min, lat_min, lon_max]

    era5_request = {
        "product_type": ["reanalysis"],
        "variable": [
            "10m_u_component_of_wind",
            "10m_v_component_of_wind"
        ],
        "year":  [year],
        "month": [month],
        "day":   days,
        "time":  times,
        "area":  area,
        "format": "netcdf"
    }


    # ====== ì €ì¥ ê²½ë¡œ ë° íŒŒì¼ëª… =====
    print("ERA5 ìš”ì²­:", era5_request)
    client = cdsapi.Client()
    wind_folder = r".\wind_data"
    os.makedirs(wind_folder, exist_ok=True)
    wind_path = os.path.join(wind_folder, f"{input_basename}_wind.nc")

    # ====== íŒŒì¼ ì¡´ì¬ ì‹œ ë‹¤ìš´ë¡œë“œ ìƒëµ ======
    if os.path.exists(wind_path):
        print(f"ğŸ”„ ì´ë¯¸ wind íŒŒì¼ ì¡´ì¬, ë‹¤ìš´ë¡œë“œ ìƒëµ: {wind_path}")
    else:
        print("ğŸŒ¬ï¸ ERA5 wind ìš”ì²­:", era5_request)
        client = cdsapi.Client()
        client.retrieve(
            'reanalysis-era5-single-levels',
            era5_request,
            wind_path
        )
        print(f"âœ… ERA5 ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {wind_path}")

    ###############################################################################
    # ê°€ì‹œê±°ë¦¬ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    ###############################################################################

    from geopy.distance import geodesic

    # ê´€ì¸¡ì†Œ ëª©ë¡ (ìœ„ë„, ê²½ë„)
    observation_stations = {
        "SF_0001": {"name": "ë¶€ì‚°í•­", "latitude": 35.091, "longitude": 129.099},
        "SF_0002": {"name": "ë¶€ì‚°í•­(ì‹ í•­)", "latitude": 35.023, "longitude": 128.808},
        "SF_0009": {"name": "í•´ìš´ëŒ€", "latitude": 35.15909, "longitude": 129.16026},
        "SF_0010": {"name": "ìš¸ì‚°í•­", "latitude": 35.501, "longitude": 129.387},
        "SF_0008": {"name": "ì—¬ìˆ˜í•­", "latitude": 34.754, "longitude": 127.752},
    }

    # API í‚¤
    service_key = 'ANM8LV6zTsRNiGg6FCUMpw=='  # ë°œê¸‰ë°›ì€ ì¸ì¦í‚¤

    # JSON íŒŒì¼ ë¡œë“œ
    json_file = geojson_file

    # fishery_behaviorê°€ 1ì¸ ë°ì´í„° ì¶”ì¶œ (ì²« ë²ˆì§¸ë§Œ)
    first_fishery_behavior = None
    for feature in data['features']:
        if feature['properties']['fishery_behavior'] == 1:
            first_fishery_behavior = feature
            break  # ì²« ë²ˆì§¸ ë°ì´í„°ë§Œ ì²˜ë¦¬

    # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ ì°¾ê¸°
    def find_closest_station(lat, lon):
        closest_station = None
        min_distance = float('inf')

        # ê° ê´€ì¸¡ì†Œì™€ì˜ ê±°ë¦¬ ê³„ì‚°
        for obs_code, station in observation_stations.items():
            station_location = (station["latitude"], station["longitude"])
            current_location = (lat, lon)
            distance = geodesic(station_location, current_location).kilometers
            
            if distance < min_distance:
                min_distance = distance
                closest_station = obs_code
        
        return closest_station
    

    # ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œì—ì„œ ê°€ì‹œê±°ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    def get_visibility_from_station(obs_code, timestamp):
        # ë‚ ì§œë§Œ ì¶”ì¶œí•´ì„œ YYYYMMDD í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        date_only = timestamp.split(" ")[0].replace("-", "")  # ë‚ ì§œë§Œ ì¶”ì¶œ (YYYYMMDD)

        # API ìš”ì²­ URL ìƒì„±
        url = f"http://www.khoa.go.kr/api/oceangrid/seafogReal/search.do" \
            f"?DataType=seafogReal" \
            f"&ServiceKey={service_key}" \
            f"&ObsCode={obs_code}" \
            f"&Date={date_only}" \
            f"&ResultType=json"
        
        # API ìš”ì²­
        response = requests.get(url)

        # ì‘ë‹µ ë°ì´í„° í™•ì¸
        if response.status_code == 200:
            data = response.json()
            
            # ì‘ë‹µ ë°ì´í„° ì¶œë ¥
            if 'result' in data and 'data' in data['result']:
                closest_time_diff = float('inf')  # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¨ì´
                closest_visibility = None

                for observation in data['result']['data']:
                    obs_time = observation['obs_time']
                    print(f"ì‘ë‹µì‹œê°„: {obs_time}")  # ì‘ë‹µ ì‹œê°„ ì¶œë ¥

                    # ì‹œê°„ ì°¨ì´ ê³„ì‚° (ë‘ ì‹œê°„ì˜ ì°¨ì´ë¥¼ ë¶„ ë‹¨ìœ„ë¡œ ê³„ì‚°)
                    try:
                        timestamp_dt = parser.parse(timestamp.strip())
                        obs_time_dt = parser.parse(obs_time.strip())
                    except Exception as e:
                        print(f"ì‹œê°„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        continue

                    time_diff = abs((timestamp_dt - obs_time_dt).total_seconds())  # ì‹œê°„ ì°¨ì´ (ì´ˆ ë‹¨ìœ„)

                    # ê°€ì¥ ê°€ê¹Œìš´ ì‹œê°„ ì°¾ê¸°
                    if time_diff < closest_time_diff:
                        closest_time_diff = time_diff
                        if 'vis' in observation:
                            closest_visibility = observation['vis']
                
                if closest_visibility:
                    return closest_visibility  # ê°€ì¥ ê°€ê¹Œìš´ ê°€ì‹œê±°ë¦¬ ë°˜í™˜
        return None

    # ì´ˆê¸° ë³€ìˆ˜
    visibility = None
    latitude = None
    longitude = None
    timestamp = None
    closest_station = None

    # fishery_behavior = 1ì¸ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì •ë³´ ì €ì¥
    if first_fishery_behavior:
        timestamp = first_fishery_behavior['properties']['time_stamp']
        latitude = first_fishery_behavior['properties']['latitude']
        longitude = first_fishery_behavior['properties']['longitude']
        closest_station = find_closest_station(latitude, longitude)
    else:
        print("fishery_behaviorê°€ 1ì¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # CSV ê²½ë¡œ ì§€ì •
    output_csv_path = r".\ê°€ì‹œê±°ë¦¬csv\visibility_log.csv"

    # 1. CSVì—ì„œ í™•ì¸
    if os.path.exists(output_csv_path):
        with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row["filename"] == geojson_filename:
                    visibility = row["visibility_m"]
                    print(f"ğŸ”„ ê¸°ì¡´ CSVì—ì„œ ê°€ì‹œê±°ë¦¬ ë¶ˆëŸ¬ì˜´: {visibility}")
                    break

    # 2. ì—†ìœ¼ë©´ API í˜¸ì¶œ
    if visibility is None  and timestamp and closest_station:
        visibility = get_visibility_from_station(closest_station, timestamp)

        if visibility:
            print(f"ì‹œê°„: {timestamp} / ìœ„ì¹˜: ({latitude}, {longitude})")
            print(f"ê°€ì¥ ê°€ê¹Œìš´ ê´€ì¸¡ì†Œ: {observation_stations[closest_station]['name']} ({closest_station})")
            print(f"ê°€ì‹œê±°ë¦¬: {visibility} ë¯¸í„°")
        else:
            print(f"ê°€ì‹œê±°ë¦¬ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 3. CSVì— ì €ì¥
    if not os.path.exists(output_csv_path):
        with open(output_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(["filename", "visibility_m"])

    # 4. ì¤‘ë³µ ì €ì¥ ë°©ì§€ í›„ ì¶”ê°€
    already_exists = False
    with open(output_csv_path, mode='r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row["filename"] == geojson_filename:
                already_exists = True
                break

    if not already_exists:
        with open(output_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow([geojson_filename, visibility if visibility else "N/A"])



    ###############################################################################
    # (4) ERA5 windíŒŒì¼ ê²½ë¡œ
    ###############################################################################
    merged_file = os.path.join(nc_folder, f"{input_basename}_uv.nc")


    wind_file = os.path.join(wind_folder, f"{input_basename}_wind.nc")


    # bottom_depth = r"C:\Users\HUFS\Desktop\opendrfit_middle-20250501T132247Z-1-001\bottom_depth.nc"
    # print("Bottom depth file:", bottom_depth)





    ###############################################################################
    # PART 3: í•´ì•ˆì„  ì½ê¸°
    ###############################################################################
    # í•´ì•ˆì„  ì½ê¸°
    coastline_file = r"C:\Users\HUFS\Desktop\2024ë…„ ì „êµ­í•´ì•ˆì„ .shp"
    coast = gpd.read_file(coastline_file)
    if coast.crs is None or coast.crs.to_string() != 'EPSG:4326':
        coast = coast.to_crs(epsg=4326)
    coast_proj = coast.to_crs(epsg=3857)
    coastal_zone = coast_proj.buffer(15000).unary_union
    coastal_zone_wgs84 = gpd.GeoSeries(coastal_zone, crs=3857).to_crs(epsg=4326).unary_union




    ###############################################################################
    # OpenDrift ëª¨ë¸ ì„¤ì • - ConnectedNetDriftë¡œ ë³€ê²½
    ###############################################################################
    class ConnectedNetDrift(OceanDrift):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.ideal_distance_m = 270  # ìë§ ì´ìƒ ê±°ë¦¬ (m)
            self.k = 0.05  # ì¡°ì • ê°•ë„ ê³„ìˆ˜ (0~1 ì‚¬ì´, ë†’ì¼ìˆ˜ë¡ ìë§ í˜•íƒœ ê°•í•¨)
            self.step = 2  # ëª‡ ê°œ ê°„ê²©ìœ¼ë¡œ ì—°ê²°í• ì§€ (3ê°œ ê°„ê²© ì—°ê²°)
            self.adjustment_loops = 2  # update ë‚´ ë°˜ë³µ ì¡°ì • íšŸìˆ˜

        def update(self):
            super().update()
            lon = self.elements.lon.copy()
            lat = self.elements.lat.copy()
            n = len(lon)

            for _ in range(self.adjustment_loops):
                for i in range(self.step, n):
                    prev_coord = (lat[i - self.step], lon[i - self.step])
                    curr_coord = (lat[i], lon[i])
                    dist = geodesic(prev_coord, curr_coord).meters
                    delta = dist - self.ideal_distance_m

                    if abs(delta) > 0.1:
                        dlat = lat[i] - lat[i - self.step]
                        dlon = lon[i] - lon[i - self.step]
                        scale = delta / dist * self.k

                        lat[i]              -= dlat * scale
                        lon[i]              -= dlon * scale
                        lat[i - self.step]  += dlat * scale
                        lon[i - self.step]  += dlon * scale

            self.elements.lon[:] = lon
            self.elements.lat[:] = lat


    o = ConnectedNetDrift(loglevel=20)
    reader_ocean = reader_netCDF_CF_generic.Reader(merged_file)
    reader_met   = reader_netCDF_CF_generic.Reader(wind_file)
    # reader_bathy = reader_netCDF_CF_generic.Reader(bottom_depth)
    o.add_reader([reader_ocean, reader_met])

    o.set_config('seed:wind_drift_factor', 0.02)
    o.set_config('drift:stokes_drift', True)
    o.set_config('general:seafloor_action', 'none')
    o.set_config('drift:vertical_advection', False)
    o.set_config('drift:vertical_mixing', False)
    o.set_config('general:coastline_action', 'previous')

    ###############################################################################
    # ì…ì ì‹œë”© (ìë§ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë”©)
    df_tumang = df[df['fishery_behavior'] == 0].copy()
    df_yangmang = df[df['fishery_behavior'] == 1].copy()
    df_tumang['time_stamp'] = pd.to_datetime(df_tumang['time_stamp'])
    df_tumang = df_tumang.sort_values('time_stamp').reset_index(drop=True)
    df_yangmang = df_yangmang.sort_values('time_stamp').reset_index(drop=True)

    for i, row in df_tumang.iterrows():
        o.seed_elements(
            lon=row['lon'],
            lat=row['lat'],
            time=row['time_stamp'],
            z=0.0,
            origin_marker=np.array([i], dtype=np.int32)
        )
        print(f"[SEED] time={row['time_stamp']}, ìœ„ì¹˜=({row['lon']}, {row['lat']})")

    ###############################################################################
    # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
    start_time_sim = df_tumang['time_stamp'].min()
    end_time_sim   = df[df['fishery_behavior'] == 1]['time_stamp'].max()
    # end_time_sim   = df_yangmang['time_stamp'].min()
    simulation_duration = end_time_sim - start_time_sim

    o.run(
        time_step=600,
        time_step_output=1800,
        duration=simulation_duration
    )


    ###############################################################################
    # ì¤‘ê°„ ì…ì ìµœì¢… ìœ„ì¹˜ ì¶”ì¶œ
    num_particles = len(df_tumang)
    mid_index = num_particles // 2  # ìë§ ì¤‘ê°„ ì‹œë”© ì…ì

    lon_traj, _ = o.get_property('lon')
    lat_traj, _ = o.get_property('lat')

    mid_lon = lon_traj[-1, mid_index].item()  # float ê°’ ì¶”ì¶œ
    mid_lat = lat_traj[-1, mid_index].item()

    print(f"[ì¤‘ê°„ ì…ì ìµœì¢… ìœ„ì¹˜] lon: {mid_lon:.5f}, lat: {mid_lat:.5f}")

    ###############################################################################
    # ì–‘ë§ êµ¬ê°„ ì¤‘ê°„ ìœ„ì¹˜ ì¶”ì¶œ
    num_yangmang = len(df_yangmang)
    if num_yangmang == 0:
        raise RuntimeError("ì–‘ë§ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    mid_index_yangmang = num_yangmang // 2
    mid_row_yangmang = df_yangmang.iloc[mid_index_yangmang]

    mid_lon_yang = mid_row_yangmang['lon']
    mid_lat_yang = mid_row_yangmang['lat']
    mid_time_yang = mid_row_yangmang['time_stamp']

    print(f"[ì–‘ë§ ì¤‘ê°„ ìœ„ì¹˜] time={mid_time_yang}, lon={mid_lon_yang:.5f}, lat={mid_lat_yang:.5f}")

    ###############################################################################
    # íˆ¬ë§ ì‹œë”© ì¤‘ê°„ ìœ„ì¹˜ ì¶”ì¶œ
    mid_row_tumang = df_tumang.iloc[mid_index]
    mid_lon_tumang = mid_row_tumang['lon']
    mid_lat_tumang = mid_row_tumang['lat']
    print(f"[íˆ¬ë§ ì¤‘ê°„ ì‹œë”© ìœ„ì¹˜] lon={mid_lon_tumang:.5f}, lat={mid_lat_tumang:.5f}")


    #####################################################################
    # ê±°ë¦¬ ê³„ì‚°
    from geopy.distance import geodesic

    point_mid_drift = (mid_lat, mid_lon)  # ì˜ˆì¸¡ëœ ìë§ ì¤‘ê°„ ì…ìì˜ ìµœì¢… ìœ„ì¹˜
    point_mid_yang = (mid_lat_yang, mid_lon_yang)  # ì‹¤ì œ ì–‘ë§ ì¤‘ê°„ ìœ„ì¹˜
    point_mid_tumang = (mid_lat_tumang, mid_lon_tumang)  # ì‹¤ì œ íˆ¬ë§ ì¤‘ê°„ ì‹œë”© ìœ„ì¹˜

    distance_km = geodesic(point_mid_drift, point_mid_yang).kilometers
    distance_tumang_yang_km = geodesic(point_mid_tumang, point_mid_yang).kilometers

    print(f"[ì¤‘ê°„ ì…ì â†” ì–‘ë§ ì¤‘ê°„ ì§€ì  ê±°ë¦¬] ì•½ {distance_km:.3f} km")
    print(f"[íˆ¬ë§ ì‹œë”© ì¤‘ê°„ â†” ì–‘ë§ ì¤‘ê°„ ê±°ë¦¬] ì•½ {distance_tumang_yang_km:.3f} km")

    ###############################################################################
    # ê°€ì‹œê±°ë¦¬ ë‹¨ìœ„ ë³€í™˜ ë° ê²°ê³¼ íŒë‹¨
    try:
        # ë¬¸ìì—´ì¼ ê²½ìš° ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
        visibility_km = float(visibility) / 1000 if visibility not in (None, "N/A", "ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨") else None
    except (ValueError, TypeError):
        visibility_km = None
    prediction_result = "íŒë‹¨ ë¶ˆê°€"
    if visibility_km is not None:
        prediction_result = "ì„±ê³µ" if distance_km < visibility_km else "ì‹¤íŒ¨"

    ###############################################################################
    # CSV ì €ì¥

    result_csv_path = r".\ì˜ˆì¸¡csv\prediction_result.csv"

    # í—¤ë” ì‘ì„±
    if not os.path.exists(result_csv_path):
        with open(result_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow([
                "filename", 
                "ì˜ˆì¸¡ê±°ë¦¬_km", 
                "ê°€ì‹œê±°ë¦¬_km", 
                "ì˜ˆì¸¡ê²°ê³¼", 
                "íˆ¬ë§ì¤‘ê°„â†”ì–‘ë§ì¤‘ê°„_km"
            ])

    # íŒŒì¼ëª… ì¶”ì¶œ
    geojson_filename = os.path.basename(geojson_file)

    # ë¬¸ìì—´ ë³€í™˜ (None ëŒ€ì‘)
    distance_str = f"{distance_km:.3f}" if distance_km is not None else "N/A"
    visibility_str = f"{visibility_km:.3f}" if visibility_km is not None else "N/A"
    tumang_yang_str = f"{distance_tumang_yang_km:.3f}" if distance_tumang_yang_km is not None else "N/A"
    result_str = prediction_result if prediction_result is not None else "íŒë‹¨ ë¶ˆê°€"


    # ê²°ê³¼ ì“°ê¸°
    with open(result_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)
        writer.writerow([
            geojson_filename, 
            distance_str, 
            visibility_str, 
            result_str, 
            tumang_yang_str
        ])

    ###############################################################################
    # ê²°ê³¼ ì‹œê°í™”


    # ê¶¤ì  ì¶”ì¶œ (OpenDrift 1.13.1 ê¸°ì¤€)
    lon_traj, _ = o.get_property('lon')
    lat_traj, _ = o.get_property('lat')

    plt.figure(figsize=(10, 7))

    # ê¶¤ì  ë¼ì¸
    for i in range(lon_traj.shape[1]):
        plt.plot(lon_traj[:, i], lat_traj[:, i], linewidth=0.5, alpha=0.3, color='gray')

    # ìµœì¢… ìœ„ì¹˜ (ë§ˆì§€ë§‰ ì‹œê°„ë‹¨ê³„)
    lon_final = lon_traj[-1, :]
    lat_final = lat_traj[-1, :]
    plt.scatter(lon_final, lat_final, s=10, color='black', alpha=0.6, label='ì‹œë®¬ë ˆì´ì…˜ ìµœì¢… ìœ„ì¹˜')

    # ì‹¤ì œ íˆ¬ë§ ë° ì–‘ë§
    plt.scatter(df_tumang['lon'], df_tumang['lat'], s=30, color='blue', marker='^', label='íˆ¬ë§ ìœ„ì¹˜ (0)')
    plt.scatter(df_yangmang['lon'], df_yangmang['lat'], s=30, color='red', marker='v', label='ì–‘ë§ ìœ„ì¹˜ (1)')
    plt.scatter(mid_lon, mid_lat, color='green', s=50, marker='*', label='ì¤‘ê°„ ì…ì ìµœì¢… ìœ„ì¹˜')
    plt.scatter(mid_lon_yang, mid_lat_yang, color='orange', s=60, marker='X', label='ì–‘ë§ ì¤‘ê°„ ì§€ì ')
    plt.scatter(mid_lon_tumang, mid_lat_tumang, color='cyan', s=60, marker='P', label='íˆ¬ë§ ì¤‘ê°„ ì§€ì ')



    # ì¶• ë²”ìœ„ ìë™ ì„¤ì • (ì „ì²´ ì¢Œí‘œ í¬í•¨)
    all_lon = np.concatenate([lon_final, df_tumang['lon'].values, df_yangmang['lon'].values])
    all_lat = np.concatenate([lat_final, df_tumang['lat'].values, df_yangmang['lat'].values])
    plt.xlim(all_lon.min() - 0.01, all_lon.max() + 0.01)
    plt.ylim(all_lat.min() - 0.01, all_lat.max() + 0.01)


    # ì˜ˆì¸¡ëœ ì¤‘ê°„ ì…ì ì¢Œí‘œ í…ìŠ¤íŠ¸ í‘œì‹œ
    plt.text(
        mid_lon + 0.005, mid_lat, 
        f"({mid_lon:.3f}, {mid_lat:.3f})", 
        fontsize=8, 
        ha='left',
        bbox=dict(boxstyle='round,pad=0.2', facecolor='white', edgecolor='gray', alpha=0.7)
    )


    plt.text(
        all_lon.max() - 0.005, all_lat.max() - 0.005,
        f"ì¤‘ê°„ ê±°ë¦¬: {distance_km:.2f} km",
        fontsize=10,
        ha='right',
        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='gray', alpha=0.7)
    )


    # ì˜ˆì¸¡ ì •ë³´ ë°•ìŠ¤ (ì˜¤ë¥¸ìª½ í•˜ë‹¨)
    info_text = f"ê°€ì‹œê±°ë¦¬: {visibility_str} km\nì˜ˆì¸¡ê²°ê³¼: {result_str}"
    plt.text(
        all_lon.min() + 0.005, all_lat.min() + 0.005,
        info_text,
        fontsize=10,
        ha='left',
        va='bottom',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='black', alpha=0.6)
    )

    # íˆ¬ë§â†”ì–‘ë§ ê±°ë¦¬ ë°•ìŠ¤ (ì™¼ìª½ ìƒë‹¨)
    tumang_yang_text = f"íˆ¬ë§â†”ì–‘ë§ ê±°ë¦¬: {distance_tumang_yang_km:.2f} km"
    plt.text(
        all_lon.min() + 0.005, all_lat.max() - 0.005,
        tumang_yang_text,
        fontsize=10,
        ha='left',
        va='top',
        bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='black', alpha=0.6)
    )

    plt.title("ì‹œë®¬ë ˆì´ì…˜ ê¶¤ì  ë° ìµœì¢… ìœ„ì¹˜ vs ì‹¤ì œ íˆ¬ë§/ì–‘ë§ ìœ„ì¹˜")
    plt.xlabel("Longitude")
    plt.ylabel("Latitude")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    # plt.show()

    # === ì‹œê°í™” ê²°ê³¼ ìë™ ì €ì¥ ===
    plot_output_dir = r".\ì‹œê°í™”ê²°ê³¼"
    os.makedirs(plot_output_dir, exist_ok=True)
    plot_filename = f"{input_basename}.png"
    plot_path = os.path.join(plot_output_dir, plot_filename)
    plt.savefig(plot_path, dpi=300)
    plt.close()


        # === ì˜ˆì¸¡ëœ ì¤‘ê°„ ìœ„ì¹˜ ì €ì¥ ===
    latlon_output_dir = r".\prediction_lat_lon"
    os.makedirs(latlon_output_dir, exist_ok=True)
    latlon_csv_path = os.path.join(latlon_output_dir, "predicted_positions.csv")

    # CSV ì—†ìœ¼ë©´ í—¤ë” ì‘ì„±
    if not os.path.exists(latlon_csv_path):
        with open(latlon_csv_path, mode='w', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow(["filename", "lat", "lon"])

    # ì¤‘ë³µ ì €ì¥ ë°©ì§€ (ê°™ì€ íŒŒì¼ëª… ì¤‘ë³µ ì €ì¥ ì•ˆ í•¨)
    already_exists = False
    with open(latlon_csv_path, mode='r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row["filename"] == geojson_filename:
                already_exists = True
                break

    if not already_exists:
        with open(latlon_csv_path, mode='a', newline='', encoding='utf-8-sig') as f:
            writer = csv.writer(f)
            writer.writerow([geojson_filename, mid_lat, mid_lon])




    print("âœ… ì™„ë£Œ")

except Exception as e:
    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {geojson_file} - {e}")
    with open(error_log_path, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow([os.path.basename(geojson_file), type(e).__name__, str(e)])